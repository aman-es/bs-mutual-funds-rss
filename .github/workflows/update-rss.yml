name: Update RSS Feed

permissions:
  contents: write

on:
  schedule:
    - cron: "0 */6 * * *"   # every 6 hours (UTC)
  workflow_dispatch:

jobs:
  update-feed:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch Mutual Fund page
        run: |
          set -e
          URL="https://www.business-standard.com/markets/mutual-fund"
          STATUS=$(curl -sS -L --compressed --http1.1 \
            -A "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" \
            -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" \
            -H "Accept-Language: en-IN,en;q=0.9" \
            -H "Referer: https://www.business-standard.com/" \
            -H "Upgrade-Insecure-Requests: 1" \
            -w "%{http_code}" -o bs.html "$URL")

          echo "HTTP_STATUS=$STATUS"
          # Treat 2xx and 3xx as OK; anything else -> show first lines and fail
          if [ "$STATUS" -ge 200 ] && [ "$STATUS" -lt 400 ]; then
            echo "Fetched page OK."
          else
            echo "Fetch failed with HTTP $STATUS. First 50 lines of response:"
            head -n 50 bs.html || true
            exit 1
          fi

          # Quick sanity: ensure the HTML has 'mutual-fund' links
          if ! grep -q "/markets/mutual-fund/" bs.html; then
            echo "Warning: page fetched but no /markets/mutual-fund/ links found. Continuing so parser fallback can try."
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install parser libs
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 lxml

      - name: Generate RSS XML (BeautifulSoup)
        run: |
          python3 - <<'PY'
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin
          from datetime import datetime, timezone, timedelta
          import html, sys

          BASE = "https://www.business-standard.com"

          # Load downloaded HTML
          with open("bs.html", "rb") as f:
            soup = BeautifulSoup(f.read(), "lxml")

          # Primary: anchors pointing to /markets/mutual-fund/
          anchors = soup.select('a[href^="/markets/mutual-fund/"], a[href*="/markets/mutual-fund/"]')

          items = []
          seen = set()

          def add_item(title, link):
            title = (title or "").strip()
            if len(title) < 10:
              return
            if link in seen:
              return
            seen.add(link)
            items.append((title, link))

          for a in anchors:
            href = a.get("href")
            if not href:
              continue
            link = urljoin(BASE, href)
            if "/markets/mutual-fund/" not in link:
              continue
            title = a.get_text(" ", strip=True)
            add_item(title, link)
            if len(items) >= 20:
              break

          # Fallback: listing blocks if primary missed
          if not items:
            for card in soup.select(".listing-txt, .list-txt, article, li"):
              a = card.select_one('a[href*="/markets/mutual-fund/"]')
              if not a:
                continue
              link = urljoin(BASE, a.get("href",""))
              title = a.get_text(" ", strip=True)
              add_item(title, link)
              if len(items) >= 20:
                break

          if not items:
            print("No items parsed from page. HTML structure may have changed.", file=sys.stderr)
            sys.exit(1)

          IST = timezone(timedelta(hours=5, minutes=30))
          now = datetime.now(IST).strftime("%a, %d %b %Y %H:%M:%S %z")

          def cdata(t): 
            return f"<![CDATA[{t}]]>"

          out = []
          out.append('<?xml version="1.0" encoding="UTF-8"?>')
          out.append('<rss version="2.0">')
          out.append('<channel>')
          out.append(f'  <title>{cdata("Business Standard â€” Mutual Funds (Unofficial)")}</title>')
          out.append('  <link>https://www.business-standard.com/markets/mutual-fund</link>')
          out.append(f'  <description>{cdata("Auto-updated feed from the Mutual Fund page")}</description>')
          out.append('  <language>en-IN</language>')
          out.append(f'  <lastBuildDate>{now}</lastBuildDate>')
          out.append('  <ttl>60</ttl>')

          for title, link in items[:15]:
            title = html.unescape(title)
            out.append('  <item>')
            out.append(f'    <title>{cdata(title)}</title>')
            out.append(f'    <link>{link}</link>')
            out.append(f'    <guid>{link}</guid>')
            out.append(f'    <pubDate>{now}</pubDate>')
            out.append(f'    <description>{cdata(title)}</description>')
            out.append('  </item>')

          out.append('</channel></rss>')

          with open("business-standard-mutual-funds.xml", "w", encoding="utf-8") as f:
            f.write("\n".join(out))

          print(f"Wrote {len(items[:15])} items.")
          PY

      - name: Commit & Push if changed
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add business-standard-mutual-funds.xml || true
          # Exit early if nothing changed
          git diff --staged --quiet && echo "No changes" && exit 0
          git commit -m "auto: refresh feed"
          git pull --rebase origin main || true
          git push origin HEAD:main
